# modules_shipments/shipments_leadtime_stats_data.py
from __future__ import annotations

import logging
import os
import json
import time
import random
import datetime as dt
import asyncio
from typing import Dict, List, Tuple, Any, Optional
import hashlib

import aiohttp
from dotenv import load_dotenv
from config_package import safe_read_json, safe_write_json

# Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
log = logging.getLogger("seller-bot.leadtime_stats_data")

# â”€â”€ paths / env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DATA_DIR = os.path.join(ROOT_DIR, "data")
CACHE_DIR = os.path.join(DATA_DIR, "cache")
CACHE_SHIP_DIR = os.path.join(CACHE_DIR, "shipments")
os.makedirs(CACHE_SHIP_DIR, exist_ok=True)

load_dotenv(os.path.join(ROOT_DIR, ".env"))

# â”€â”€ settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEAD_STAT_DAYS_DEFAULT = int(os.getenv("LEAD_STAT_DAYS", "180"))
LEAD_STAT_TTL_HOURS = int(os.getenv("LEAD_STAT_TTL_HOURS", "12"))

LEAD_STATS_PREFS_PATH = os.path.join(CACHE_DIR, "common", "lead_stats_prefs.json")
os.makedirs(os.path.dirname(LEAD_STATS_PREFS_PATH), exist_ok=True)

EVENTS_CACHE_PATH = os.path.join(CACHE_SHIP_DIR, "leadtime_events.json")
STATS_CACHE_PATH = os.path.join(CACHE_SHIP_DIR, "leadtime_stats.json")

# ÐºÑÑˆ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ ÑÑ‚Ð°Ñ‚ÑƒÑÐ¾Ð² (Ñ„Ð°Ð·Ð° B)
STATES_CACHE_PATH = os.path.join(CACHE_SHIP_DIR, "leadtime_states.json")

LEAD_DISABLE_INGEST_ON_READ = bool(int(os.getenv("LEAD_DISABLE_INGEST_ON_READ", "1")))

# API creds/limits
OZON_CLIENT_ID = os.getenv("OZON_CLIENT_ID", "")
OZON_API_KEY = os.getenv("OZON_API_KEY", "")

LEAD_FETCH_BATCH = int(os.getenv("LEAD_FETCH_BATCH", "100"))
LEAD_GET_BATCH = int(os.getenv("LEAD_GET_BATCH", "50"))
LEAD_BUNDLE_MAX_PER_RUN = int(os.getenv("LEAD_BUNDLE_MAX_PER_RUN", "15"))
LEAD_BUNDLE_BASE_PAUSE_SEC = float(os.getenv("LEAD_BUNDLE_BASE_PAUSE_SEC", "0.4"))
LEAD_BUNDLE_MAX_TOTAL_TRIES = int(os.getenv("LEAD_BUNDLE_MAX_TOTAL_TRIES", "60"))
LEAD_RETENTION_DAYS = int(os.getenv("LEAD_RETENTION_DAYS", "360"))
LEAD_MAX_PAGES = int(os.getenv("LEAD_MAX_PAGES", "50"))

# HTTP throttling
LEAD_HTTP_TIMEOUT = int(os.getenv("LEAD_HTTP_TIMEOUT", "20"))
LEAD_RETRY_AFTER_CAP = float(os.getenv("LEAD_RETRY_AFTER_CAP", "2.5"))

# Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ (ÑˆÑƒÐ¼Ð¾Ð²Ð¾Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³)
LEAD_MIN_DAYS = float(os.getenv("LEAD_MIN_DAYS", "0.0"))

# ingest tick / state
LEAD_INGEST_INTERVAL_SEC = int(os.getenv("LEAD_INGEST_INTERVAL_SEC", "900"))
LEAD_INGEST_PAGES_DEFAULT = int(os.getenv("LEAD_INGEST_PAGES", "3"))
LEAD_INGEST_STATE_PATH = os.path.join(CACHE_SHIP_DIR, "leadtime_ingest_state.json")

# Ð¿ÐµÑ€Ð²Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ð¾Ð½: Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ð°Ñ Ð³Ð»ÑƒÐ±Ð¸Ð½Ð°
LEAD_PRIMARY_PAGES = int(os.getenv("LEAD_PRIMARY_PAGES", "5"))

# Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ñ-Ð·Ð°Ð¿ÑƒÑÐº Ñ‚Ð¸ÐºÐ° (Ð¸Ð³Ð½Ð¾Ñ€ Ð°Ð½Ñ‚Ð¸Ð´Ñ€ÐµÐ±ÐµÐ·Ð³Ð°)
LEAD_TICK_FORCE = bool(int(os.getenv("LEAD_TICK_FORCE", "0")))


LEAD_STAT_PERIODS = (90, 180, 360)

# â”€â”€ WATCH_SKU: Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð¸ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð´Ð»Ñ Â«Ð¿Ð¾ SKUÂ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAW_WATCH_SCU = os.getenv("WATCH_SKU", "") or ""


def _parse_watch_sku(raw: str) -> List[int]:
    """
    Ð Ð°Ð·Ð±Ð¾Ñ€ WATCH_SKU Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² '123' Ð¸ '123:alias'.
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð¸ ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸.
    """
    txt = (raw or "").replace("\n", ",").replace(" ", ",")
    out: List[int] = []
    seen: set[int] = set()
    for tok in [t.strip() for t in txt.split(",") if t.strip()]:
        left = tok.split(":", 1)[0].strip()
        try:
            v = int(left)
        except Exception:
            continue
        if v not in seen:
            seen.add(v)
            out.append(v)
    return out


WATCH_ORDER: List[int] = _parse_watch_sku(RAW_WATCH_SCU)
WATCH_POS = {sku: i for i, sku in enumerate(WATCH_ORDER)}
WATCH_SET = set(WATCH_ORDER)


def get_current_watch_sku() -> List[int]:
    """ÐŸÑƒÐ±Ð»Ð¸Ñ‡Ð½Ð°Ñ ÑƒÑ‚Ð¸Ð»Ð¸Ñ‚Ð° Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸: Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº WATCH_SKU."""
    return list(WATCH_ORDER)


def _order_key_for_sku(sku: int, alias: str = "") -> Tuple[int, str]:
    """ÐšÐ»ÑŽÑ‡ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸: Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ñ Ð² WATCH_SKU, Ð¿Ð¾Ñ‚Ð¾Ð¼ alias (Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸)."""
    return (WATCH_POS.get(int(sku), 10**9), (alias or "").lower())


# â”€â”€ tiny json utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _read_json(path: str) -> dict:
    """Ð§Ð¸Ñ‚Ð°ÐµÑ‚ JSON Ñ„Ð°Ð¹Ð» Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼."""
    return safe_read_json(path)


def _write_json(path: str, payload: dict) -> None:
    """Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ JSON Ñ„Ð°Ð¹Ð» Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼."""
    safe_write_json(path, payload)


# â”€â”€ time helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _parse_iso_dt(s: str) -> Optional[dt.datetime]:
    if not s:
        return None
    try:
        d = dt.datetime.fromisoformat(str(s).replace("Z", "+00:00"))
        return d if d.tzinfo else d.replace(tzinfo=dt.timezone.utc)
    except Exception:
        return None


def _iso_ge(a: str, b: str) -> bool:
    """
    True, ÐµÑÐ»Ð¸ ISOâ€‘ÑÑ‚Ñ€Ð¾ÐºÐ° a >= b (ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ 'Z'/Ñ‚Ð°Ð¹Ð¼Ð·Ð¾Ð½Ñ‹).
    Ð•ÑÐ»Ð¸ b Ð¿ÑƒÑÑ‚Ð°Ñ â€” ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑƒÑÐ»Ð¾Ð²Ð¸Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ð¼.
    """
    if not b:
        return True
    da = _parse_iso_dt(a)
    db = _parse_iso_dt(b)
    if da and db:
        return da >= db
    return str(a) >= str(b)


def _events_saved_at() -> str:
    try:
        d = _read_json(EVENTS_CACHE_PATH)
        return str(d.get("saved_at") or "")
    except Exception:
        return ""


def _is_events_empty() -> bool:
    d = _read_json(EVENTS_CACHE_PATH)
    return not bool(d.get("rows"))


def _utc_now_iso() -> str:
    return dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00", "Z")


# â”€â”€ prefs (period + allocation flag) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _load_prefs() -> dict:
    d = _read_json(LEAD_STATS_PREFS_PATH)
    period = int(d.get("period", LEAD_STAT_DAYS_DEFAULT))
    if period not in LEAD_STAT_PERIODS:
        period = LEAD_STAT_DAYS_DEFAULT
    alloc = bool(d.get("allocate_by_qty", True))
    return {"period": period, "allocate_by_qty": alloc}


def get_stat_period() -> int:
    return int(_load_prefs().get("period", LEAD_STAT_DAYS_DEFAULT))


def save_stat_period(period: int) -> None:
    if period not in LEAD_STAT_PERIODS:
        return
    cur = _load_prefs()
    cur["period"] = int(period)
    _write_json(LEAD_STATS_PREFS_PATH, cur)


def get_lead_allocation_flag() -> bool:
    return bool(_load_prefs().get("allocate_by_qty", True))


# â”€â”€ Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð»Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def set_lead_allocation_flag(flag: bool) -> None:
    """
    ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ Ñ„Ð»Ð°Ð³ Â«ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð²ÐµÑ Ð¿Ð°Ñ€Ñ‚Ð¸Ð¸Â», Ð¿Ð¾ÑÐ»Ðµ Ñ‡ÐµÐ³Ð¾:
        â€¢ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿ÐµÑ€ÐµÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÑ‚ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¸Ð· states Ñ Ð½Ð¾Ð²Ñ‹Ð¼ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾Ð¼,
        â€¢ Ð¸Ð½Ð²Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ ÐºÑÑˆ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸.
    """
    cur = _load_prefs()
    cur["allocate_by_qty"] = bool(flag)
    _write_json(LEAD_STATS_PREFS_PATH, cur)
    try:
        _write_json(EVENTS_CACHE_PATH, {"saved_at": _utc_now_iso(), "rows": [], "version": 2})
        _emit_phase_b_events_from_states(_utc_now_iso())
        _write_json(STATS_CACHE_PATH, {})
    except Exception:
        # Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ ÑÐ±Ñ€Ð¾ÑÐ¸Ð¼ ÐºÑÑˆ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
        try:
            _write_json(STATS_CACHE_PATH, {})
        except Exception:
            pass


# â”€â”€ freshness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _is_fresh(saved_iso: str, ttl_hours: int) -> bool:
    ts = _parse_iso_dt(saved_iso)
    if not ts:
        return False
    now = dt.datetime.now(dt.timezone.utc)
    return (now - ts) <= dt.timedelta(hours=max(1, int(ttl_hours)))


# â”€â”€ materialize & filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _materialize_events(period_days: int) -> List[dict]:
    d = _read_json(EVENTS_CACHE_PATH)
    rows = d.get("rows", []) if isinstance(d, dict) else []
    if not rows:
        return []
    cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=max(1, int(period_days)))
    out: List[dict] = []
    for e in rows:
        try:
            if str(e.get("phase") or "") != "post_dropoff":
                continue
            tend = _parse_iso_dt(str(e.get("ts_end", "")))
            if tend and tend >= cutoff:
                out.append(e)
        except Exception:
            continue
    return out


def _only_completed_with_duration(events: List[dict]) -> List[dict]:
    out: List[dict] = []
    for e in events:
        try:
            dur_val = e.get("duration_days")
            if isinstance(dur_val, (int, float)) and float(dur_val) > 0:
                dur = float(dur_val)
            else:
                a = _parse_iso_dt(str(e.get("ts_start", "")))
                b = _parse_iso_dt(str(e.get("ts_end", "")))
                if not a or not b or b <= a:
                    continue
                dur = (b - a).total_seconds() / 86400.0

            if LEAD_MIN_DAYS <= dur <= 90:
                ee = dict(e)
                ee["duration_days"] = float(dur)
                out.append(ee)
        except Exception:
            continue
    return out


# â”€â”€ stats helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _percentile(vals: List[float], p: float) -> float:
    if not vals:
        return 0.0
    arr = sorted(vals)
    n = len(arr)
    if n == 1:
        return float(arr[0])
    if p <= 0:
        return float(arr[0])
    if p >= 1:
        return float(arr[-1])
    k = (n - 1) * float(p)
    f = int(k)
    c = f + 1
    if c >= n:
        return float(arr[-1])
    if f == c:
        return float(arr[f])
    d = k - f
    return float(arr[f] * (1.0 - d) + arr[c] * d)


def _aggregate_stats(events: List[dict], key_fn) -> List[Tuple[Any, Dict[str, float]]]:
    buckets: Dict[Any, List[float]] = {}
    for e in events:
        k = key_fn(e)
        if k is None:
            continue
        buckets.setdefault(k, []).append(float(e.get("duration_days", 0.0)))
    out: List[Tuple[Any, Dict[str, float]]] = []
    for k, arr in buckets.items():
        arr = [float(x) for x in arr if x is not None]
        if not arr:
            continue
        arr.sort()
        n = len(arr)
        stats = {
            "avg": sum(arr) / n,
            "p50": _percentile(arr, 0.5),
            "p90": _percentile(arr, 0.9),
            "min": arr[0],
            "max": arr[-1],
            "n": float(n),
        }
        out.append((k, stats))
    return out


# â”€â”€ HTTP helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _ozon_headers() -> Dict[str, str]:
    return {
        "Client-Id": OZON_CLIENT_ID,
        "Api-Key": OZON_API_KEY,
        "Content-Type": "application/json",
    }


async def _post_json(url: str, payload: dict, timeout: Optional[int] = None):
    timeout_obj = aiohttp.ClientTimeout(connect=5, total=(timeout or LEAD_HTTP_TIMEOUT))
    async with aiohttp.ClientSession(timeout=timeout_obj) as session:
        try:
            async with session.post(url, headers=_ozon_headers(), json=payload) as r:
                try:
                    js = await r.json()
                except Exception:
                    js = {}
                if r.status == 429:
                    return js, r
                r.raise_for_status()
                return js, r
        except Exception:
            return {}, None


async def _respect_rate_limit_sleep(resp):
    try:
        if hasattr(resp, "status") and int(resp.status) == 429:
            ra = resp.headers.get("Retry-After")
            delay = float(ra) if ra is not None else 1.0
            delay = max(0.0, min(delay, LEAD_RETRY_AFTER_CAP))
            await asyncio.sleep(delay + random.uniform(0, 0.2))
    except Exception:
        await asyncio.sleep(0.5 + random.uniform(0, 0.2))


# â”€â”€ v2â†’v3: Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÑ‚Ð°Ñ‚ÑƒÑÐ¾Ð² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ÐšÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð½Ð°Ð±Ð¾Ñ€ (v3)
START_STATE = "ACCEPTED_AT_SUPPLY_WAREHOUSE"
END_STATE = "REPORTS_CONFIRMATION_AWAITING"
COMP_STATE = "COMPLETED"
# ðŸ†• ÐŸÑ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Â«ÐºÐ¾Ð½ÐµÑ‡Ð½Ð°ÑÂ» ÑÑ‚Ð°Ð´Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚Ð° lead time
STORAGE_ACCEPT_STATE = "ACCEPTANCE_AT_STORAGE_WAREHOUSE"

# ÐÐ»Ð¸Ð°ÑÑ‹ v2 â†’ v3
_STATE_ALIASES = {
    "ORDER_STATE_ACCEPTED_AT_SUPPLY_WAREHOUSE": START_STATE,
    "ORDER_STATE_IN_TRANSIT": "IN_TRANSIT",
    "ORDER_STATE_ACCEPTANCE_AT_STORAGE_WAREHOUSE": STORAGE_ACCEPT_STATE,
    "ORDER_STATE_REPORTS_CONFIRMATION_AWAITING": END_STATE,
    "ORDER_STATE_COMPLETED": COMP_STATE,
    "ORDER_STATE_DATA_FILLING": "DATA_FILLING",
    "ORDER_STATE_READY_TO_SUPPLY": "READY_TO_SUPPLY",
    "ORDER_STATE_REJECTED_AT_SUPPLY_WAREHOUSE": "REJECTED_AT_SUPPLY_WAREHOUSE",
    "ORDER_STATE_CANCELLED": "CANCELLED",
    "ORDER_STATE_OVERDUE": "OVERDUE",
}


def _normalize_state(name: str) -> str:
    s = str(name or "").strip()
    up = s.upper()
    return _STATE_ALIASES.get(up, up)


def _js_pick(d: dict, *keys) -> Any:
    """
    Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð¸Ð· Ð¸Ð¼Ñ‘Ð½ ÐºÐ»ÑŽÑ‡ÐµÐ¹.
    ÐŸÑ‹Ñ‚Ð°ÐµÑ‚ÑÑ Ñ‚Ð°ÐºÐ¶Ðµ Ð·Ð°Ð³Ð»ÑÐ½ÑƒÑ‚ÑŒ Ð² Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ðµ Ð¾Ð±Ñ‘Ñ€Ñ‚ÐºÐ¸ ('result', 'data').
    """
    if not isinstance(d, dict):
        return None
    for k in keys:
        if k in d:
            return d.get(k)
    for wrap in ("result", "data"):
        sub = d.get(wrap)
        if isinstance(sub, dict):
            for k in keys:
                if k in sub:
                    return sub.get(k)
    return None


# â”€â”€ supply-order API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def _supply_list(
    states: List[str], from_id: int = 0, limit: int = 100
) -> Tuple[List[str], int]:
    """
    v3: POST /v3/supply-order/list
      payload: {"filter":{"states":[...]}, "paging":{"from_supply_order_id":<int>|"from_order_id":<int>, "limit":<=100}}
      response (Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹): {"order_ids":[...], "last_supply_order_id":<int>} â€” Ð¸Ð»Ð¸ â€” {"supply_order_id":[...], ...}
    Ð’ÑÑ‚Ñ€Ð¾ÐµÐ½ Ñ„Ð¾Ð»Ð±ÑÐº: ÐµÑÐ»Ð¸ ÑÐµÑ€Ð²ÐµÑ€ Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ»ÑŽÑ‡ 'from_supply_order_id' Ð¸ Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ñ Â«ÑÑ‚Ð¾Ð¸Ñ‚ Ð½Ð° Ð¼ÐµÑÑ‚ÐµÂ»,
    Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ 'from_order_id'.
    """
    # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÑÑ‚Ð°Ñ‚ÑƒÑÐ¾Ð² Ðº v3
    states_v3 = []
    seen = set()
    for s in states or []:
        norm = _normalize_state(s)
        if norm and norm not in seen:
            seen.add(norm)
            states_v3.append(norm)

    async def _call_with(paging_key: str) -> dict:
        payload = {
            "filter": {"states": states_v3},
            "paging": {paging_key: int(from_id), "limit": min(int(limit), 100)},
        }
        js, resp = await _post_json("https://api-seller.ozon.ru/v3/supply-order/list", payload)
        if resp is not None and getattr(resp, "status", 200) == 429:
            await _respect_rate_limit_sleep(resp)
            js, _ = await _post_json("https://api-seller.ozon.ru/v3/supply-order/list", payload)
        return js or {}

    def _parse(js: dict) -> Tuple[List[str], int]:
        ids = _js_pick(js, "order_ids", "supply_order_id", "ids")
        if not isinstance(ids, list):
            ids = []
        nxt = _js_pick(js, "last_supply_order_id", "last_order_id", "last_id")
        try:
            nxt_i = int(nxt) if nxt is not None else 0
        except Exception:
            nxt_i = 0
        out_ids: List[str] = []
        for x in ids:
            try:
                out_ids.append(str(int(x)))
            except Exception:
                continue
        return out_ids, nxt_i

    # 1) Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð²Ñ‹Ð·Ð¾Ð² â€” ÑÑ‚Ð°Ñ€Ð¾Ðµ Ð¿Ð¾Ð»Ðµ from_supply_order_id
    js1 = await _call_with("from_supply_order_id")
    ids1, nxt1 = _parse(js1)

    # 2) Ñ„Ð¾Ð»Ð±ÑÐº â€” Ð½Ð¾Ð²Ð¾Ðµ/Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ðµ from_order_id (ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ° Ð½ÐµÑ‚)
    use_fallback = from_id > 0 and (nxt1 <= from_id)
    if use_fallback:
        js2 = await _call_with("from_order_id")
        ids2, nxt2 = _parse(js2)
        # Ð±ÐµÑ€Ñ‘Ð¼ Ñ‚Ð¾Ñ‚ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð´Ð°Ñ‘Ñ‚ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð²Ð¿ÐµÑ€Ñ‘Ð´ Ð¸Ð»Ð¸ Ð½ÐµÐ¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº
        if ids2 or (nxt2 > nxt1 and nxt2 != from_id):
            return ids2, nxt2

    return ids1, nxt1


async def _supply_get(order_ids: List[str]) -> List[dict]:
    if not order_ids:
        return []
    payload = {"order_ids": [str(x) for x in order_ids[:50]]}
    js, resp = await _post_json("https://api-seller.ozon.ru/v3/supply-order/get", payload)
    if resp is not None and getattr(resp, "status", 200) == 429:
        await _respect_rate_limit_sleep(resp)
        js, _ = await _post_json("https://api-seller.ozon.ru/v3/supply-order/get", payload)
    orders = _js_pick(js, "orders")
    return orders if isinstance(orders, list) else []


# â”€â”€ status semantics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ÐšÐ¾Ð½ÐµÑ‡Ð½Ð°Ñ Ñ„Ð°Ð·Ð° Ð´Ð»Ñ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚Ð° lead time: Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚
# 1) ACCEPTANCE_AT_STORAGE_WAREHOUSE â†’ 2) REPORTS_CONFIRMATION_AWAITING â†’ 3) COMPLETED


def _get_end_ts_from_states(states: Dict[str, str]) -> Optional[str]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ISOâ€‘Ð²Ñ€ÐµÐ¼Ñ Â«Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸ÑÂ» Ð·Ð°ÑÐ²ÐºÐ¸.
    ÐŸÐ¾ Ð½Ð¾Ð²Ð¾Ð¼Ñƒ Ð¢Ð—: Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð½Ð¾ Ð±ÐµÑ€Ñ‘Ð¼ ACCEPTANCE_AT_STORAGE_WAREHOUSE,
    ÐµÑÐ»Ð¸ ÐµÑ‘ Ð½ÐµÑ‚ â€” REPORTS_CONFIRMATION_AWAITING, Ð´Ð°Ð»ÐµÐµ â€” COMPLETED.
    """
    if not isinstance(states, dict):
        return None
    return states.get(STORAGE_ACCEPT_STATE) or states.get(END_STATE) or states.get(COMP_STATE)


def _has_end_state(states: Dict[str, str]) -> bool:
    s = states or {}
    return bool(s.get(STORAGE_ACCEPT_STATE) or s.get(END_STATE) or s.get(COMP_STATE))


# â”€â”€ status cache (Ñ„Ð°Ð·Ð° B) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _states_load() -> dict:
    d = _read_json(STATES_CACHE_PATH)
    return d if isinstance(d, dict) else {}


def _states_save(payload: dict) -> None:
    _write_json(STATES_CACHE_PATH, payload or {})


def _ensure_list_unique_int(lst) -> List[int]:
    out: List[int] = []
    for v in lst or []:
        try:
            i = int(v)
            if i not in out:
                out.append(i)
        except Exception:
            continue
    return out


def _bundle_items(bundle_ids: List[str]) -> Dict[str, List[Tuple[int, float]]]:
    """Ð§Ñ‚ÐµÐ½Ð¸Ðµ ÑÐ¾ÑÑ‚Ð°Ð²Ð° Ð·Ð°ÑÐ²ÐºÐ¸ {bundle_id: [(sku, qty), ...]} Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒÑŽ."""
    out: Dict[str, List[Tuple[int, float]]] = {}
    if not bundle_ids or not requests:
        return out
    todo = [str(bid).strip() for bid in bundle_ids if str(bid).strip()]
    if len(todo) > LEAD_BUNDLE_MAX_PER_RUN:
        todo = todo[:LEAD_BUNDLE_MAX_PER_RUN]
    total_tries = 0
    for bid in todo:
        if total_tries >= LEAD_BUNDLE_MAX_TOTAL_TRIES:
            break
        bucket: List[Tuple[int, float]] = []
        last_id: Optional[str] = None
        has_next = True
        while has_next:
            payload = {"bundle_ids": [bid], "limit": 100, "is_asc": True}
            if last_id:
                payload["last_id"] = last_id
            attempt = 0
            while attempt < 3:
                attempt += 1
                total_tries += 1
                js, resp = _post_json(
                    "https://api-seller.ozon.ru/v1/supply-order/bundle",
                    payload,
                    timeout=LEAD_HTTP_TIMEOUT,
                )
                if resp is not None and getattr(resp, "status_code", 200) == 429:
                    _respect_rate_limit_sleep(resp)
                    continue
                items = _js_pick(js, "items")
                items = items if isinstance(items, list) else []
                for it in items:
                    sku = it.get("sku")
                    qty = it.get("quantity", 1)
                    try:
                        sku_i = int(sku)
                        q_f = float(qty if qty is not None else 1.0)
                        if q_f <= 0:
                            q_f = 1.0
                        bucket.append((sku_i, q_f))
                    except Exception:
                        continue
                has_next = bool(js.get("has_next"))
                last_id = js.get("last_id") if has_next else None
                time.sleep(0.06)
                break
            time.sleep(0.03)
        if bucket:
            out[bid] = bucket
        time.sleep(0.03)
    return out


def _states_upsert_from_get(orders: List[dict], now_iso: Optional[str] = None) -> None:
    """
    ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÐºÑÑˆ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð¿Ð¾ ÑÐ¿Ð¸ÑÐºÑƒ orders Ð¸Ð· /v3/supply-order/get.

    Ð’ÐÐ–ÐÐž:
    - ÐÐ• ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¸ ÐÐ• Ð¾Ñ‚Ð¼ÐµÑ‡Ð°ÐµÐ¼ Â«Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸ÐµÂ», ÐµÑÐ»Ð¸ Ñƒ Ð·Ð°ÑÐ²ÐºÐ¸ ÐµÑ‰Ñ‘ Ð½ÐµÑ‚ ACCEPTED (ÑÐ¼. Ð½Ð¸Ð¶Ðµ purge).
    - Ð¡Ð½Ð¸Ð¼Ð¾Ðº sku_items Ð´ÐµÐ»Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¸ ÐŸÐ•Ð Ð’ÐžÐœ ACCEPTED.
    - ÐšÐ¾Ð½ÐµÑ‡Ð½Ð°Ñ Ð´Ð°Ñ‚Ð° Ð´Ð»Ñ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚Ð° lead time Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº:
      ACCEPTANCE_AT_STORAGE_WAREHOUSE â†’ REPORTS_CONFIRMATION_AWAITING â†’ COMPLETED.
    """
    if not orders:
        return
    now_iso = now_iso or _utc_now_iso()
    cache = _states_load()

    order_to_bundles: Dict[str, List[str]] = {}

    for o in orders:
        # ID Ð·Ð°ÑÐ²ÐºÐ¸: v3 â€” order_id; v2 â€” supply_order_id
        sid_i: Optional[int] = None
        try:
            sid_i = int(o.get("order_id"))
        except Exception:
            try:
                sid_i = int(o.get("supply_order_id"))
            except Exception:
                sid_i = None
        if sid_i is None:
            continue
        sid = str(sid_i)

        st_name = _normalize_state(o.get("state") or "")

        # Ð•ÑÐ»Ð¸ Ð·Ð°ÑÐ²ÐºÐ° Ð½Ð°Ð¼ ÐµÑ‰Ñ‘ Ð½Ðµ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð° Ð¸ ÑÑ‚Ð¾ Â«Ð³Ð¾Ð»Ð¾Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸ÐµÂ» â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼
        if sid not in cache and st_name in {COMP_STATE, END_STATE, STORAGE_ACCEPT_STATE}:
            continue

        rec = cache.get(sid) or {}
        if not rec.get("first_seen"):
            rec["first_seen"] = now_iso

        # dropoff (v3: dropoff_warehouse.warehouse_id |
        # drop_off_warehouse.warehouse_id; v2: dropoff_warehouse_id)
        drop_wid = None
        try:
            drop_wid = int(o.get("dropoff_warehouse_id"))
        except Exception:
            try:
                drop_w = (
                    o.get("dropoff_warehouse") or o.get("drop_off_warehouse") or {}
                )  # â† Ð´Ð¾Ð¿. Ñ„Ð¾Ð»Ð±ÑÐº
                drop_wid = int((drop_w or {}).get("warehouse_id"))
            except Exception:
                drop_wid = None
        rec["dropoff_wid"] = drop_wid if drop_wid else rec.get("dropoff_wid") or None

        # storage_wids + bundle_ids
        cur_wids = rec.get("storage_wids") or []
        found_wids: List[int] = []
        bundles: List[str] = []
        for s in o.get("supplies") or []:
            # v3: storage_warehouse.warehouse_id; v2: storage_warehouse_id
            wid = None
            try:
                wid = int(s.get("storage_warehouse_id"))
            except Exception:
                try:
                    wid = int((s.get("storage_warehouse") or {}).get("warehouse_id"))
                except Exception:
                    wid = None
            if wid:
                found_wids.append(wid)
            bid = s.get("bundle_id")
            if bid:
                bundles.append(str(bid))

        rec["storage_wids"] = _ensure_list_unique_int(cur_wids + found_wids)
        if bundles:
            rec["bundle_ids"] = sorted(
                list({*([str(b) for b in bundles]), *([str(b) for b in rec.get("bundle_ids",                                                                                   [])])})
            )

        # Ð½Ð¾Ð¼ÐµÑ€ Ð·Ð°ÑÐ²ÐºÐ¸ (Ð² v3: order_number; v2: supply_order_number)
        so_num = o.get("order_number") or o.get("supply_order_number")
        if so_num and not rec.get("supply_order_number"):
            rec["supply_order_number"] = str(so_num)

        # Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐµÐ¼ Â«Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð·Ð°Ð¼ÐµÑ‡ÐµÐ½Ð½Ð¾Ðµ Ð²Ñ€ÐµÐ¼ÑÂ» Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹
        states: Dict[str, str] = rec.get("states") or {}

        if st_name == START_STATE:
            if START_STATE not in states:
                states[START_STATE] = now_iso
                if rec.get("bundle_ids"):
                    order_to_bundles[sid] = rec["bundle_ids"]

        elif st_name == END_STATE:
            # Â«Ð¡Ð¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð°ÐºÑ‚Ð¾Ð²Â» â€” Ð¾Ð´Ð½Ð° Ð¸Ð· Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÑ‚Ð°Ð´Ð¸Ð¹
            if END_STATE not in states:
                states[END_STATE] = now_iso

        elif st_name == COMP_STATE:
            # Ð¤Ð¾Ð»Ð±ÑÐº (ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚ÑŒ Ðº ÑÑ‚Ð°Ñ€Ñ‹Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼/Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°Ð¼)
            if START_STATE in states and COMP_STATE not in states:
                states[COMP_STATE] = now_iso

        else:
            # ÑÑŽÐ´Ð° Ð¿Ð¾Ð¿Ð°Ð´Ð°ÐµÑ‚ Ð¸ ACCEPTANCE_AT_STORAGE_WAREHOUSE (Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÐºÐ¾Ð½ÐµÑ‡Ð½Ð°Ñ)
            if st_name and st_name not in states:
                states[st_name] = now_iso

        rec["states"] = states
        cache[sid] = rec

    # Ð¡Ð½Ð¸Ð¼Ð¾Ðº sku_items Ð½Ð° Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ACCEPTED
    if order_to_bundles:
        for sid, bundle_ids in order_to_bundles.items():
            try:
                bmap = _bundle_items(bundle_ids or [])
                sku_items: List[Tuple[int, float]] = []
                for bid in bundle_ids or []:
                    for pair in bmap.get(bid) or []:
                        sku_items.append(pair)
                if sku_items:
                    rec = cache.get(sid) or {}
                    if not rec.get("sku_items"):  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð´Ð¸Ð½ ÑÐ½Ð¸Ð¼Ð¾Ðº
                        rec["sku_items"] = [[int(s), float(q)] for s, q in sku_items]
                        cache[sid] = rec
            except Exception:
                continue

    _states_save(cache)
    try:
        _purge_completed_without_start()
    except Exception:
        pass


# â”€â”€ retention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _retain_events(now_utc: Optional[dt.datetime] = None) -> int:
    now_utc = now_utc or dt.datetime.now(dt.timezone.utc)
    cutoff = now_utc - dt.timedelta(days=max(1, int(LEAD_RETENTION_DAYS)))
    cache = _read_json(EVENTS_CACHE_PATH)
    rows = cache.get("rows", []) if isinstance(cache, dict) else []
    kept: List[dict] = []
    removed = 0
    for e in rows:
        try:
            tend = _parse_iso_dt(str(e.get("ts_end", "")))
            if tend and tend < cutoff:
                removed += 1
                continue
            kept.append(e)
        except Exception:
            kept.append(e)
    if removed:
        _write_json(EVENTS_CACHE_PATH, {"saved_at": _utc_now_iso(), "rows": kept, "version": 2})
        try:
            _write_json(STATS_CACHE_PATH, {})
        except Exception:
            pass
    return removed


def _retain_states(now_utc: Optional[dt.datetime] = None) -> int:
    now_utc = now_utc or dt.datetime.now(dt.timezone.utc)
    cutoff = now_utc - dt.timedelta(days=max(1, int(LEAD_RETENTION_DAYS)))
    cache = _states_load()
    removed = 0
    for sid, rec in list(cache.items()):
        try:
            states = rec.get("states") or {}
            cmp_iso = _get_end_ts_from_states(states)
            if not cmp_iso:
                continue
            tend = _parse_iso_dt(str(cmp_iso))
            if tend and tend < cutoff:
                cache.pop(sid, None)
                removed += 1
        except Exception:
            continue
    if removed:
        _states_save(cache)
    return removed


def _purge_completed_without_start() -> int:
    """
    Ð£Ð´Ð°Ð»ÑÐµÑ‚ Ð¸Ð· ÐºÑÑˆÐ° Ð·Ð°ÑÐ²ÐºÐ¸, Ð³Ð´Ðµ ÐµÑÑ‚ÑŒ Â«ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ð¹Â» ÑÑ‚Ð°Ñ‚ÑƒÑ
    (ACCEPTANCE_AT_STORAGE_WAREHOUSE/REPORTS_CONFIRMATION_AWAITING/COMPLETED),
    Ð½Ð¾ ÐÐ•Ð¢ ACCEPTED.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‡Ð¸ÑÐ»Ð¾ ÑƒÐ´Ð°Ð»Ñ‘Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹.
    """
    cache = _states_load()
    removed = 0
    for sid, rec in list(cache.items()):
        st = rec.get("states") or {}
        if _has_end_state(st) and START_STATE not in st:
            cache.pop(sid, None)
            removed += 1
    if removed:
        _states_save(cache)
    return removed


# â”€â”€ ingest state helpers (fix NameError) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _default_ingest_state() -> dict:
    return {
        "last_run_at": "",
        "last_added": 0,
        "last_pages": 0,
        "next_allowed_ts": 0.0,
        "is_running": False,
    }


def _read_state() -> dict:
    """
    Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¸Ð½Ð¶ÐµÑÑ‚Ð° Ð¸Ð· LEAD_INGEST_STATE_PATH.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ ÑÐ¾ Ð²ÑÐµÐ¼Ð¸ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸.
    """
    d = _read_json(LEAD_INGEST_STATE_PATH)
    if not isinstance(d, dict):
        d = {}
    state = _default_ingest_state()
    state.update({k: d.get(k, state[k]) for k in state.keys()})
    # Ñ‚Ð¸Ð¿Ð¾Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ñ
    try:
        state["last_added"] = int(state.get("last_added") or 0)
    except Exception:
        state["last_added"] = 0
    try:
        state["last_pages"] = int(state.get("last_pages") or 0)
    except Exception:
        state["last_pages"] = 0
    try:
        state["next_allowed_ts"] = float(state.get("next_allowed_ts") or 0.0)
    except Exception:
        state["next_allowed_ts"] = 0.0
    state["is_running"] = bool(state.get("is_running", False))
    state["last_run_at"] = str(state.get("last_run_at") or "")
    return state


def _write_state(payload: dict) -> None:
    """
    ÐÑ‚Ð¾Ð¼Ð°Ñ€Ð½Ð°Ñ Ð·Ð°Ð¿Ð¸ÑÑŒ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¸Ð½Ð¶ÐµÑÑ‚Ð° Ð² LEAD_INGEST_STATE_PATH.
    """
    st = _default_ingest_state()
    try:
        st.update(payload or {})
    except Exception:
        pass
    _write_json(LEAD_INGEST_STATE_PATH, st)


def _should_force_tick(now_ts: float, st: dict, primary_bootstrap: bool) -> bool:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ True, ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ñ€Ð¾Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð°Ð½Ñ‚Ð¸Ð´Ñ€ÐµÐ±ÐµÐ·Ð³ Ð¸ ÑÑ‚Ð°Ñ€Ñ‚Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚Ð¸Ðº Ð½ÐµÐ¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾:
      â€¢ Ð²ÐºÐ»ÑŽÑ‡Ñ‘Ð½ LEAD_TICK_FORCE,
      â€¢ Ð¿ÐµÑ€Ð²Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð±ÑƒÑ‚ÑÑ‚Ñ€Ð°Ð¿ (Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÐºÑÑˆ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹).
    """
    try:
        if bool(int(os.getenv("LEAD_TICK_FORCE", "0"))):
            return True
    except Exception:
        pass
    if primary_bootstrap:
        return True
    return False


# â”€â”€ emit phase-B events from states â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _wid_to_cluster_map() -> Dict[int, int]:
    """
    ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¿ÑƒÑ‚ÑŒ â€” Ð¸Ð· stocks (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½). Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ â€” Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„Ð¾Ð»Ð±ÑÐºÐ¸:
    â€¢ mapping Ð¸Ð· shipments_leadtime.get_warehouse_cluster_map()
    â€¢ /cluster/list â†’ warehouse_id â†’ cluster_name â†’ ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ int id (ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸).
    """
    wid2cid: Dict[int, int] = {}
    # 1) stocks (view=warehouse)
    try:
        from modules_shipments.shipments_data import fetch_stocks_view  # type: ignore

        for r in fetch_stocks_view(view="warehouse") or []:
            try:
                wid = int(r.get("warehouse_id") or (r.get("dimensions") or [{}])[0].get("id"))
                cid = int(
                    r.get("cluster_id")
                    or (r.get("dimensions") or [{}])[0].get("cluster_id")
                    or (r.get("dimensions") or [{}])[0].get("clusterId")
                )
                wid2cid[wid] = cid
            except Exception:
                continue
    except Exception:
        pass
    if wid2cid:
        return wid2cid

    # 2) ÑÐ²Ð½Ð°Ñ ÐºÐ°Ñ€Ñ‚Ð° Ð¸Ð· leadtime (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
    try:
        from .shipments_leadtime import get_warehouse_cluster_map  # type: ignore

        m = get_warehouse_cluster_map() or {}
        for w, c in (m or {}).items():
            try:
                wid2cid[int(w)] = int(c)
            except Exception:
                continue
        if wid2cid:
            return wid2cid
    except Exception:
        pass

    # 3) Ð¤ÐžÐ›Ð‘Ð­Ðš: /v1/cluster/list â†’ warehouse_id â†’ cluster_name â†’ ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ id
    try:
        from .shipments_report_data import load_clusters  # type: ignore

        js = load_clusters(force=False) or {}
        name_by_wid: Dict[int, str] = {}
        for cl in js.get("clusters") or []:
            cname = (
                cl.get("name") or cl.get("title") or cl.get("cluster_name") or ""
            ).strip() or "ÐšÐ»Ð°ÑÑ‚ÐµÑ€"
            for lc in cl.get("logistic_clusters") or []:
                for wh in lc.get("warehouses") or []:
                    wid = wh.get("warehouse_id") or wh.get("id") or wh.get("warehouseId")
                    try:
                        name_by_wid[int(wid)] = cname
                    except Exception:
                        continue
        if name_by_wid:
            out: Dict[int, int] = {}
            for wid, cname in name_by_wid.items():
                h = int(hashlib.md5(cname.encode("utf-8")).hexdigest()[:8], 16)
                out[int(wid)] = int(h & 0x7FFFFFFF)
            return out
    except Exception:
        pass
    return {}


def _emit_phase_b_events_from_states(now_iso: Optional[str] = None) -> int:
    """
    Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð·Ð°ÑÐ²ÐºÐ¸ Ñ ACCEPTED Ð¸ Â«Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸ÐµÐ¼Â»
    (ACCEPTANCE_AT_STORAGE_WAREHOUSE/REPORTS_CONFIRMATION_AWAITING/COMPLETED)
    ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼:
      â€¢ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð½ÑƒÑŽ Ð³Ñ€Ð°Ð½ÑƒÐ»Ñƒ (sku=None) â€” Ð’Ð¡Ð•Ð“Ð”Ð, ÐµÑÐ»Ð¸ Ð¸Ð·Ð²ÐµÑÑ‚ÐµÐ½ ÑÐºÐ»Ð°Ð´ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ,
      â€¢ SKUâ€‘Ð³Ñ€Ð°Ð½ÑƒÐ»Ñ‹ (sku=int) â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ½ÑÑ‚ ÑÐ½Ð¸Ð¼Ð¾Ðº sku_items.
    """
    now_iso = now_iso or _utc_now_iso()
    states = _states_load()
    if not states:
        return 0

    wid2cid = _wid_to_cluster_map()

    prev = _read_json(EVENTS_CACHE_PATH)
    rows_prev = prev.get("rows", []) if isinstance(prev, dict) else []

    def _key(e: dict) -> Tuple[int, int, str, Optional[int]]:
        sid = int(e.get("supply_order_id") or 0)
        wid = int(e.get("storage_wid") or 0)
        ts = str(e.get("ts_end") or "")
        sku = e.get("sku")
        try:
            sku_i = int(sku) if sku is not None else None
        except Exception:
            sku_i = None
        return (sid, wid, ts, sku_i)

    seen = {_key(e) for e in rows_prev}

    cutoff_keep = dt.datetime.now(dt.timezone.utc) - dt.timedelta(
        days=max(1, int(LEAD_RETENTION_DAYS))
    )

    new_events: List[dict] = []
    added = 0

    for sid, rec in states.items():
        try:
            sid_i = int(sid)
        except Exception:
            continue
        st = rec.get("states") or {}
        ts_start = st.get(START_STATE)
        ts_end = _get_end_ts_from_states(st)
        if not (ts_start and ts_end):
            continue

        a = _parse_iso_dt(str(ts_start))
        b = _parse_iso_dt(str(ts_end))
        if not a or not b or b <= a:
            continue
        dur = (b - a).total_seconds() / 86400.0
        if not (LEAD_MIN_DAYS <= dur <= 90):
            continue
        if b < cutoff_keep:
            continue

        dropoff_wid = rec.get("dropoff_wid")
        storage_wids = rec.get("storage_wids") or []
        sku_items = rec.get("sku_items") or []

        if not storage_wids:
            continue

        has_sku_items = bool(sku_items)
        total_q = sum((float(q or 0.0) for _, q in sku_items)) if has_sku_items else 0.0
        alloc_by_qty = get_lead_allocation_flag()

        for storage_i in storage_wids:
            try:
                wid_i = int(storage_i)
            except Exception:
                continue

            base = {
                "phase": "post_dropoff",
                "supply_order_id": sid_i,
                "ts_start": ts_start,
                "ts_end": ts_end,
                "dropoff_wid": int(dropoff_wid) if dropoff_wid else None,
                "storage_wid": wid_i,
                "cluster_id": wid2cid.get(wid_i),
                "cluster_name": None,
                "supply_order_number": rec.get("supply_order_number"),
                "source": "supply_order",
                "quality": "phase_b",
                "duration_days": float(dur),
            }

            # 1) ÐÐ³Ñ€ÐµÐ³Ð°Ñ‚Ð½Ð°Ñ Ð³Ñ€Ð°Ð½ÑƒÐ»Ð° â€” Ð²ÑÐµÐ³Ð´Ð°
            k0 = (sid_i, wid_i, ts_end, None)
            if k0 not in seen:
                new_events.append(dict(base, **{"sku": None, "qty": None}))
                seen.add(k0)
                added += 1

            # 2) SKUâ€‘Ð³Ñ€Ð°Ð½ÑƒÐ»Ñ‹ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¸ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ð¸ ÑÐ½Ð¸Ð¼ÐºÐ° ÑÐ¾ÑÑ‚Ð°Ð²Ð°
            if has_sku_items:
                tq = total_q if total_q > 0 else 1.0
                for sku, qty in sku_items:
                    try:
                        sku_i = int(sku)
                        qty_f = float(qty or 0.0)
                    except Exception:
                        continue
                    part_days = float(dur) if not alloc_by_qty else float(dur) * (qty_f / tq)
                    e2 = dict(base)
                    e2["sku"] = sku_i
                    e2["qty"] = float(qty_f)
                    e2["duration_days"] = float(part_days)
                    k = (sid_i, wid_i, ts_end, sku_i)
                    if k in seen:
                        continue
                    new_events.append(e2)
                    seen.add(k)
                    added += 1

    if not added:
        return 0

    merged = rows_prev + new_events
    _write_json(EVENTS_CACHE_PATH, {"saved_at": _utc_now_iso(), "rows": merged, "version": 2})
    try:
        _write_json(STATS_CACHE_PATH, {})
    except Exception:
        pass
    return added


# â”€â”€ ensure recent on read â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _ensure_recent_events(period_days: int, max_pages: int) -> None:
    if LEAD_DISABLE_INGEST_ON_READ:
        return
    ev = _materialize_events(period_days)
    if ev:
        return
    try:
        update_leadtime_events(
            days=int(period_days),
            pages=max(1, int(max_pages)),
            primary_bootstrap=_is_events_empty(),
        )
    except Exception as ex:
        print("[leadtime] ensure_recent_events soft-failed:", ex)


# â”€â”€ stats cache helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _load_stats_cache() -> dict:
    return _read_json(STATS_CACHE_PATH)


def _save_stats_cache(key: str, payload: Any) -> None:
    allc = _load_stats_cache()
    if not isinstance(allc, dict):
        allc = {}
    allc[key] = {"saved_at": _utc_now_iso(), "payload": payload}
    _write_json(STATS_CACHE_PATH, allc)


def _stats_key(period: int, view: str) -> str:
    # Ð¤Ð»Ð°Ð³ Ð°Ð»Ð»Ð¾ÐºÐ°Ñ†Ð¸Ð¸ Ð²Ð»Ð¸ÑÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° "sku"
    alloc = (
        "1" if (view == "sku" and get_lead_allocation_flag()) else ("0" if view == "sku" else "-")
    )
    return f"P{int(period)}:{str(view)}:alloc={alloc}"


# â”€â”€ helpers: cluster names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _extract_cluster_name_from_row(r: dict) -> Tuple[Optional[int], str]:
    cid = r.get("cluster_id")
    name_candidates = [
        r.get("cluster_name"),
        r.get("cluster"),
        r.get("clusterTitle"),
        r.get("cluster_title"),
        r.get("name"),
        r.get("title"),
    ]
    dims = r.get("dimensions") or []
    if dims and isinstance(dims, list) and isinstance(dims[0], dict):
        cid = cid or dims[0].get("cluster_id") or dims[0].get("clusterId")
        name_candidates.extend(
            [
                dims[0].get("cluster_name"),
                dims[0].get("cluster"),
                dims[0].get("clusterTitle"),
                dims[0].get("cluster_title"),
                dims[0].get("name"),
                dims[0].get("title"),
            ]
        )
    cname = ""
    for c in name_candidates:
        s = str(c or "").strip()
        if s:
            cname = s
            break
    try:
        cid_i = int(cid) if cid is not None else None
    except Exception:
        cid_i = None
    return cid_i, cname


def _cluster_names_by_id() -> Dict[int, str]:
    """
    Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ {cluster_id -> name} Ð¸Ð· Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²:
      â€¢ stocks(view='warehouse')
      â€¢ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ð¹ _get_stocks(view='warehouse')
      â€¢ /cluster/list (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ id Ñƒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°)
    """
    out: Dict[int, str] = {}

    # 1) stocks(view='warehouse')
    try:
        from modules_shipments.shipments_data import fetch_stocks_view  # type: ignore

        for r in fetch_stocks_view(view="warehouse") or []:
            cid, cname = _extract_cluster_name_from_row(r)
            if cid and cname and cid not in out:
                out[int(cid)] = str(cname)
    except Exception:
        pass

    # 2) leadtime._get_stocks(view='warehouse')
    if not out:
        try:
            from .shipments_leadtime import _get_stocks  # type: ignore

            for r in _get_stocks(view="warehouse") or []:
                cid, cname = _extract_cluster_name_from_row(r)
                if cid and cname and cid not in out:
                    out[int(cid)] = str(cname)
        except Exception:
            pass

    # 3) /cluster/list (id â†’ name)
    if not out:
        try:
            from .shipments_report_data import load_clusters  # type: ignore

            js = load_clusters(force=False) or {}
            for cl in js.get("clusters") or []:
                cid = cl.get("id") or cl.get("cluster_id") or cl.get("clusterId")
                cname = (cl.get("name") or cl.get("title") or cl.get("cluster_name") or "").strip()
                try:
                    if cid is not None and cname:
                        out[int(cid)] = cname
                except Exception:
                    continue
        except Exception:
            pass

    return out


# â”€â”€ public stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_lead_stats_summary(period_days: int | None = None) -> Dict[str, float]:
    period = int(period_days or get_stat_period())
    key = _stats_key(period, "summary")
    cache_all = _load_stats_cache()
    cache = cache_all.get(key) or {}
    stats_saved = str(cache.get("saved_at") or "")
    events_saved = _events_saved_at()
    if cache and _is_fresh(stats_saved, LEAD_STAT_TTL_HOURS) and _iso_ge(stats_saved,                                                                             events_saved):
        return cache.get("payload", {})
    _ensure_recent_events(period, max_pages=2)
    events = _only_completed_with_duration(_materialize_events(period))
    # Ð¡Ð²Ð¾Ð´ÐºÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸ÑÐ¼ (Ð¾Ð´Ð¸Ð½ Ð·Ð°ÐºÐ°Ð· Ã— Ð¾Ð´Ð¸Ð½ ÑÐºÐ»Ð°Ð´)
    events = [e for e in events if (e.get("sku") is None or int(e.get("sku") or 0) == 0)]
    vals = [float(e["duration_days"]) for e in events]
    if not vals:
        payload = {"avg": 0.0, "p50": 0.0, "p90": 0.0, "n": 0.0}
        _save_stats_cache(key, payload)
        return payload
    vals.sort()
    n = len(vals)
    payload = {
        "avg": sum(vals) / n,
        "p50": _percentile(vals, 0.5),
        "p90": _percentile(vals, 0.9),
        "n": float(n),
    }
    _save_stats_cache(key, payload)
    return payload


def get_lead_stats_by_warehouse(
    period_days: int | None = None,
) -> List[Tuple[int, str, Dict[str, float]]]:
    period = int(period_days or get_stat_period())
    key = _stats_key(period, "warehouse")
    cache_all = _load_stats_cache()
    cache = cache_all.get(key) or {}
    stats_saved = str(cache.get("saved_at") or "")
    events_saved = _events_saved_at()
    if cache and _is_fresh(stats_saved, LEAD_STAT_TTL_HOURS) and _iso_ge(stats_saved,                                                                             events_saved):
        return cache.get("payload", [])
    _ensure_recent_events(period, max_pages=2)

    events = _only_completed_with_duration(_materialize_events(period))
    # ÐÐ³Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸ÑÐ¼ â€” Ð¾Ð´Ð½Ð¾ Ð½Ð° Ð·Ð°ÐºÐ°Ð·Ã—ÑÐºÐ»Ð°Ð´
    events = [e for e in events if (e.get("sku") is None or int(e.get("sku") or 0) == 0)]

    if not events:
        _save_stats_cache(key, [])
        return []
    try:
        from .shipments_leadtime import get_current_warehouses  # type: ignore

        wid_name = get_current_warehouses()
    except Exception:
        wid_name = {}
    try:
        from .shipments_leadtime_data import get_warehouse_title as _wh_title_fallback  # type: ignore
    except Exception:

        def _wh_title_fallback(wid: int) -> str:
            return f"wh:{wid}"

    aggr = _aggregate_stats(events, key_fn=lambda e: int(e.get("storage_wid") or 0) or None)
    out = []
    for wid, m in aggr:
        try:
            wid_i = int(wid)
        except Exception:
            continue
        title = wid_name.get(wid_i) or _wh_title_fallback(wid_i) or f"wh:{wid_i}"
        out.append((wid_i, title, m))
    out.sort(
        key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), str(t[1]).lower())
    )
    _save_stats_cache(key, out)
    return out


def get_lead_stats_by_cluster(
    period_days: int | None = None,
) -> List[Tuple[int, str, Dict[str, float]]]:
    period = int(period_days or get_stat_period())
    key = _stats_key(period, "cluster")
    cache_all = _load_stats_cache()
    cache = cache_all.get(key) or {}
    stats_saved = str(cache.get("saved_at") or "")
    events_saved = _events_saved_at()
    if cache and _is_fresh(stats_saved, LEAD_STAT_TTL_HOURS) and _iso_ge(stats_saved,                                                                             events_saved):
        return cache.get("payload", [])
    _ensure_recent_events(period, max_pages=2)

    events = _only_completed_with_duration(_materialize_events(period))
    events = [e for e in events if (e.get("sku") is None or int(e.get("sku") or 0) == 0)]

    if not events:
        _save_stats_cache(key, [])
        return []

    # ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¿ÑƒÑ‚ÑŒ: Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ id ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° (Ñ‡ÐµÑ€ÐµÐ· map ÑÐºÐ»Ð°Ð´â†’ÐºÐ»Ð°ÑÑ‚ÐµÑ€)
    try:
        from .shipments_leadtime import get_warehouse_cluster_map, _get_stocks  # type: ignore

        wid2cid = get_warehouse_cluster_map()
        cid_name: Dict[int, str] = {}
        # Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð´Ð¾ÑÑ‚Ð°Ñ‚ÑŒ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ð¸Ð· stocks leadtime
        for r in _get_stocks(view="warehouse") or []:
            try:
                cid, cname = _extract_cluster_name_from_row(r)
                if cid and cname and cid not in cid_name:
                    cid_name[cid] = cname
            except Exception:
                continue
    except Exception:
        wid2cid, cid_name = {}, {}

    # Ð”Ð¾Ð¿. Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¸Ð¼Ñ‘Ð½ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð², ÐµÑÐ»Ð¸ Ð¿ÑƒÑÑ‚Ð¾/Ð½ÐµÐ¿Ð¾Ð»Ð½Ð¾
    if wid2cid and not cid_name:
        try:
            cid_name = _cluster_names_by_id()
        except Exception:
            cid_name = {}

    if wid2cid:
        aggr = _aggregate_stats(
            events, key_fn=lambda e: wid2cid.get(int(e.get("storage_wid") or 0))
        )
        out: List[Tuple[int, str, Dict[str, float]]] = []
        for cid, m in aggr:
            if cid is None:
                continue
            cname = cid_name.get(int(cid))
            if not cname:
                cname = f"ÐšÐ»Ð°ÑÑ‚ÐµÑ€ {int(cid)}"
            out.append((int(cid), cname, m))
        out.sort(
            key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), str(t[1]).lower())
        )
        _save_stats_cache(key, out)
        return out

    # Ð¤ÐžÐ›Ð‘Ð­Ðš â€” Ð±ÐµÐ· widâ†’cid: Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° Ð¸Ð· /cluster/list
    try:
        from .shipments_report_data import load_clusters  # type: ignore

        js = load_clusters(force=False) or {}
        name_by_wid: Dict[int, str] = {}
        for cl in js.get("clusters") or []:
            cname = (
                cl.get("name") or cl.get("title") or cl.get("cluster_name") or ""
            ).strip() or "ÐšÐ»Ð°ÑÑ‚ÐµÑ€"
            for lc in cl.get("logistic_clusters") or []:
                for wh in lc.get("warehouses") or []:
                    wid = wh.get("warehouse_id") or wh.get("id") or wh.get("warehouseId")
                    try:
                        name_by_wid[int(wid)] = cname
                    except Exception:
                        continue
    except Exception:
        name_by_wid = {}

    if not name_by_wid:
        _save_stats_cache(key, [])
        return []

    aggr2 = _aggregate_stats(
        events, key_fn=lambda e: name_by_wid.get(int(e.get("storage_wid") or 0))
    )
    out2: List[Tuple[int, str, Dict[str, float]]] = []
    for cname, m in aggr2:
        if not cname:
            continue
        cid_synth = int(hashlib.md5(str(cname).encode("utf-8")).hexdigest()[:8], 16) & 0x7FFFFFFF
        out2.append((cid_synth, str(cname), m))
    out2.sort(
        key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), str(t[1]).lower())
    )
    _save_stats_cache(key, out2)
    return out2


def get_lead_stats_by_sku(
    period_days: int | None = None,
) -> List[Tuple[int, str, Dict[str, float]]]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ð¾ SKU Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð¼/Ð¿Ð¾Ñ€ÑÐ´ÐºÐ¾Ð¼ Ð¿Ð¾ WATCH_SKU (ÐµÑÐ»Ð¸ Ð·Ð°Ð´Ð°Ð½).
    Ð•ÑÐ»Ð¸ WATCH_SKU Ð·Ð°Ð´Ð°Ð½, Ð½Ð¾ Ð½Ðµ Ð´Ð°Ð» Ð¿Ð¾Ð¿Ð°Ð´Ð°Ð½Ð¸Ð¹ â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ„Ð¾Ð»Ð±ÑÐº Ðº Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼.
    """
    period = int(period_days or get_stat_period())
    key = _stats_key(period, "sku")
    cache_all = _load_stats_cache()
    cache = cache_all.get(key) or {}
    stats_saved = str(cache.get("saved_at") or "")
    events_saved = _events_saved_at()
    if cache and _is_fresh(stats_saved, LEAD_STAT_TTL_HOURS) and _iso_ge(stats_saved,                                                                             events_saved):
        return cache.get("payload", [])
    _ensure_recent_events(period, max_pages=2)
    events = _only_completed_with_duration(_materialize_events(period))
    events = [e for e in events if int(e.get("sku") or 0) > 0]
    if not events:
        _save_stats_cache(key, [])
        return []
    try:
        from modules_sales.sales_facts_store import get_alias_for_sku  # type: ignore
    except Exception:

        def get_alias_for_sku(sku: int) -> str:  # type: ignore
            return str(sku)

    aggr = _aggregate_stats(events, key_fn=lambda e: int(e.get("sku") or 0) or None)
    aggr_map: Dict[int, Dict[str, float]] = {}
    for sku, m in aggr:
        try:
            aggr_map[int(sku)] = dict(m or {})
        except Exception:
            continue

    out: List[Tuple[int, str, Dict[str, float]]] = []
    if WATCH_ORDER:
        for sku in WATCH_ORDER:
            m = aggr_map.get(int(sku))
            if not m:
                continue
            alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
            out.append((int(sku), alias, m))
        if not out:
            tmp: List[Tuple[int, str, Dict[str, float]]] = []
            for sku, m in aggr:
                alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
                tmp.append((int(sku), alias, m))
            tmp.sort(
                key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), t[1].lower())
            )
            out = tmp
    else:
        tmp: List[Tuple[int, str, Dict[str, float]]] = []
        for sku, m in aggr:
            alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
            tmp.append((int(sku), alias, m))
        tmp.sort(key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), t[1].lower()))
        out = tmp

    _save_stats_cache(key, out)
    return out


# â”€â”€ drill-down helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_lead_stats_sku_for_warehouse(
    warehouse_id: int, period_days: int | None = None
) -> List[Tuple[int, str, Dict[str, float]]]:
    period = int(period_days or get_stat_period())
    _ensure_recent_events(period, max_pages=2)
    events = _only_completed_with_duration(_materialize_events(period))
    ev = [
        e
        for e in events
        if int(e.get("sku") or 0) > 0 and int(e.get("storage_wid") or 0) == int(warehouse_id)
    ]
    if not ev:
        return []
    try:
        from modules_sales.sales_facts_store import get_alias_for_sku  # type: ignore
    except Exception:

        def get_alias_for_sku(sku: int) -> str:  # type: ignore
            return str(sku)

    aggr = _aggregate_stats(ev, key_fn=lambda e: int(e.get("sku") or 0) or None)

    aggr_map: Dict[int, Dict[str, float]] = {}
    for sku, m in aggr:
        try:
            aggr_map[int(sku)] = dict(m or {})
        except Exception:
            continue

    out: List[Tuple[int, str, Dict[str, float]]] = []
    if WATCH_ORDER:
        for sku in WATCH_ORDER:
            m = aggr_map.get(int(sku))
            if m:
                alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
                out.append((int(sku), alias, m))
        if not out:
            tmp: List[Tuple[int, str, Dict[str, float]]] = []
            for sku, m in aggr:
                alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
                tmp.append((int(sku), alias, m))
            tmp.sort(
                key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), t[1].lower())
            )
            out = tmp
    else:
        tmp: List[Tuple[int, str, Dict[str, float]]] = []
        for sku, m in aggr:
            alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
            tmp.append((int(sku), alias, m))
        tmp.sort(key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), t[1].lower()))
        out = tmp

    return out


def get_lead_stats_sku_for_cluster(
    cluster_id: int, period_days: int | None = None
) -> List[Tuple[int, str, Dict[str, float]]]:
    period = int(period_days or get_stat_period())
    _ensure_recent_events(period, max_pages=2)
    events = _only_completed_with_duration(_materialize_events(period))
    ev = [
        e
        for e in events
        if int(e.get("sku") or 0) > 0 and int(e.get("cluster_id") or 0) == int(cluster_id)
    ]
    if not ev:
        return []
    try:
        from modules_sales.sales_facts_store import get_alias_for_sku  # type: ignore
    except Exception:

        def get_alias_for_sku(sku: int) -> str:  # type: ignore
            return str(sku)

    aggr = _aggregate_stats(ev, key_fn=lambda e: int(e.get("sku") or 0) or None)

    aggr_map: Dict[int, Dict[str, float]] = {}
    for sku, m in aggr:
        try:
            aggr_map[int(sku)] = dict(m or {})
        except Exception:
            continue

    out: List[Tuple[int, str, Dict[str, float]]] = []
    if WATCH_ORDER:
        for sku in WATCH_ORDER:
            m = aggr_map.get(int(sku))
            if m:
                alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
                out.append((int(sku), alias, m))
        if not out:
            tmp: List[Tuple[int, str, Dict[str, float]]] = []
            for sku, m in aggr:
                alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
                tmp.append((int(sku), alias, m))
            tmp.sort(
                key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), t[1].lower())
            )
            out = tmp
    else:
        tmp: List[Tuple[int, str, Dict[str, float]]] = []
        for sku, m in aggr:
            alias = (get_alias_for_sku(int(sku)) or "").strip() or str(sku)
            tmp.append((int(sku), alias, m))
        tmp.sort(key=lambda t: (-int(t[2].get("n", 0)), -float(t[2].get("avg", 0.0)), t[1].lower()))
        out = tmp

    return out


# â”€â”€ helpers for manual leads sync â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def get_stats_avg_by_warehouse(period_days: int) -> Dict[int, float]:
    out: Dict[int, float] = {}
    for wid, _name, metrics in get_lead_stats_by_warehouse(period_days):
        try:
            out[int(wid)] = float(metrics.get("avg", 0.0) or 0.0)
        except Exception:
            continue
    return out


def apply_stats_to_leads_for_followers() -> int:
    try:
        from modules_shipments.shipments_leadtime_data import get_following_wids, set_lead_for_wid  # type: ignore
    except Exception:
        return 0

    followers = get_following_wids() or {}
    if not followers:
        return 0

    periods = sorted({int((rec or {}).get("follow_period") or 90) for rec in followers.values()})
    period_maps: Dict[int, Dict[int, float]] = {}
    for p in periods:
        try:
            period_maps[p] = get_stats_avg_by_warehouse(int(p))
        except Exception:
            period_maps[p] = {}

    updated = 0
    for wid, rec in followers.items():
        try:
            p = int((rec or {}).get("follow_period") or 90)
            metric = str((rec or {}).get("follow_metric") or "avg")
            if metric != "avg":
                continue
            avg = float(period_maps.get(p, {}).get(int(wid)) or 0.0)
            if avg <= 0:
                continue
            set_lead_for_wid(int(wid), round(avg, 2), updated_by=f"stats_sync:P{p}")
            updated += 1
        except Exception:
            continue
    return updated


# ðŸ†• â”€â”€ Ð°Ð²Ñ‚Ð¾â€‘Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸ Ð´Ð»Ñ ÑÐºÐ»Ð°Ð´Ð¾Ð², Ð·Ð°Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð² ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _auto_enable_follow_for_seen_wids() -> int:
    try:
        from modules_shipments.shipments_leadtime_data import get_following_wids, enable_follow_stats  # type: ignore
    except Exception:
        return 0
    try:
        existing = set(int(w) for w in (get_following_wids() or {}).keys())
    except Exception:
        existing = set()

    js = _read_json(EVENTS_CACHE_PATH)
    rows = js.get("rows", []) if isinstance(js, dict) else []
    seen: set[int] = set()
    for e in rows:
        try:
            if str(e.get("phase") or "") != "post_dropoff":
                continue
            w = int(e.get("storage_wid") or 0)
            if w > 0:
                seen.add(w)
        except Exception:
            continue

    todo = [w for w in seen if w not in existing]
    if not todo:
        return 0

    period = get_stat_period() or LEAD_STAT_DAYS_DEFAULT
    enabled = 0
    for w in todo:
        try:
            enable_follow_stats(int(w), period=int(period), metric="avg")
            enabled += 1
        except Exception:
            continue
    return enabled


def update_leadtime_events(
    days: int = LEAD_STAT_DAYS_DEFAULT,
    source: str = "all",
    pages: int = 1,
    *,
    primary_bootstrap: bool = False,
) -> int:
    if not requests or not OZON_CLIENT_ID or not OZON_API_KEY:
        print("[leadtime] requests or API keys missing; skip ingest")
        return 0

    _retain_events()
    _retain_states()

    from_id = 0
    pages_limit = max(1, min(int(pages), int(LEAD_MAX_PAGES)))
    page_cnt = 0
    # v3 ÑÑ‚Ð°Ñ‚ÑƒÑÑ‹
    STATES = [
        START_STATE,
        "IN_TRANSIT",
        STORAGE_ACCEPT_STATE,  # â† ÐºÐ»ÑŽÑ‡ÐµÐ²Ð°Ñ Â«ÐºÐ¾Ð½ÐµÑ‡Ð½Ð°ÑÂ» ÑÑ‚Ð°Ð´Ð¸Ñ
        END_STATE,  # â† Ñ„Ð¾Ð»Ð±ÑÐº 1
        COMP_STATE,  # â† Ñ„Ð¾Ð»Ð±ÑÐº 2
    ]

    while page_cnt < pages_limit:
        ids, nxt = _supply_list(
            states=STATES, from_id=from_id, limit=min(int(LEAD_FETCH_BATCH), 100)
        )
        if not ids:
            if page_cnt == 0:
                print("[leadtime] no supply-order ids (multi-status)")
            break
        page_cnt += 1

        for i in range(0, len(ids), max(1, int(LEAD_GET_BATCH))):
            batch_ids = ids[i : i + max(1, int(LEAD_GET_BATCH))]
            orders = _supply_get(batch_ids)
            if orders:
                try:
                    _states_upsert_from_get(orders, now_iso=_utc_now_iso())
                except Exception as ex:
                    print("[leadtime] states_upsert error:", ex)
            time.sleep(0.06)

        # Ð·Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ ÑÑ‚Ð°Ð³Ð½Ð°Ñ†Ð¸Ð¸ Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ ÑÐµÑ€Ð²ÐµÑ€ Ð½Ðµ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚ from_* Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€)
        if not nxt or int(nxt) <= int(from_id):
            break
        from_id = nxt
        time.sleep(0.08)

    try:
        _purge_completed_without_start()
    except Exception:
        pass

    added = 0
    try:
        added = _emit_phase_b_events_from_states(_utc_now_iso())
    except Exception as ex:
        print("[leadtime] emit phase-B events failed:", ex)
        added = 0

    if added > 0:
        try:
            _write_json(STATS_CACHE_PATH, {})
        except Exception:
            pass

    # ðŸ†• Ð°Ð²Ñ‚Ð¾â€‘Ð²ÐºÐ»ÑŽÑ‡Ð¸Ð¼ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð¿Ð¾ÑÐ²Ð¸Ð²ÑˆÐ¸Ñ…ÑÑ ÑÐºÐ»Ð°Ð´Ð¾Ð² Ð¸ ÑÑ€Ð°Ð·Ñƒ Ð¿Ð¾Ð´Ñ‚ÑÐ½ÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
    try:
        newly_enabled = _auto_enable_follow_for_seen_wids()
        if newly_enabled:
            print(f"[leadtime] auto-follow enabled for {newly_enabled} warehouses")
    except Exception as ex:
        print("[leadtime] auto-follow failed:", ex)

    try:
        synced = apply_stats_to_leads_for_followers()
        if synced:
            print(f"[leadtime] stats_sync: updated manual leads for {synced} followers")
    except Exception as ex:
        print("[leadtime] stats_sync failed:", ex)

    print(f"[leadtime] phase-B events added: {int(added)}")
    return int(added or 0)


def ingest_tick(pages: Optional[int] = None, days: Optional[int] = None) -> int:
    st = _read_state()
    now = dt.datetime.now().timestamp()

    primary_bootstrap = _is_events_empty()

    if not _should_force_tick(now, st, primary_bootstrap):
        if now < float(st.get("next_allowed_ts") or 0.0) or st.get("is_running"):
            return 0

    st["is_running"] = True
    _write_state(st)
    try:
        period_days = int(days or get_stat_period() or LEAD_STAT_DAYS_DEFAULT)

        page_depth = int(pages if pages is not None else LEAD_INGEST_PAGES_DEFAULT)
        if primary_bootstrap:
            page_depth = max(page_depth, int(os.getenv("LEAD_PRIMARY_PAGES", LEAD_PRIMARY_PAGES)))
        page_depth = max(1, min(page_depth, int(os.getenv("LEAD_MAX_PAGES", "50"))))

        added = update_leadtime_events(
            days=period_days, pages=page_depth, primary_bootstrap=primary_bootstrap
        )

        try:
            _write_json(STATS_CACHE_PATH, {})
        except Exception:
            pass

        st.update(
            {
                "last_run_at": dt.datetime.now().isoformat(timespec="seconds"),
                "last_added": int(added or 0),
                "last_pages": page_depth,
                "next_allowed_ts": now + max(60, int(LEAD_INGEST_INTERVAL_SEC)),
                "is_running": False,
            }
        )
        _write_state(st)
        return int(added or 0)
    except Exception:
        st.update(
            {
                "last_run_at": dt.datetime.now().isoformat(timespec="seconds"),
                "last_added": 0,
                "last_pages": int(pages or LEAD_INGEST_PAGES_DEFAULT),
                "next_allowed_ts": now + max(60, int(LEAD_INGEST_INTERVAL_SEC)),
                "is_running": False,
            }
        )
        _write_state(st)
        return 0


# â”€â”€ maintenance (public) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def invalidate_stats_cache() -> None:
    try:
        _write_json(STATS_CACHE_PATH, {})
    except Exception:
        pass


def rebuild_events_from_states() -> int:
    try:
        _write_json(EVENTS_CACHE_PATH, {"saved_at": _utc_now_iso(), "rows": [], "version": 2})
        added = _emit_phase_b_events_from_states(_utc_now_iso())
        _write_json(STATS_CACHE_PATH, {})
        # ðŸ†• ÑÑ€Ð°Ð·Ñƒ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ð¼ follow Ð´Ð»Ñ Ð·Ð°Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ñ… ÑÐºÐ»Ð°Ð´Ð¾Ð² Ð¸ Ð¿Ð¾Ð´Ñ‚ÑÐ½ÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
        try:
            newly_enabled = _auto_enable_follow_for_seen_wids()
            if newly_enabled:
                print(f"[leadtime] auto-follow enabled for {newly_enabled} warehouses (rebuild)")
        except Exception as ex:
            print("[leadtime] auto-follow failed (rebuild):", ex)

        try:
            synced = apply_stats_to_leads_for_followers()
            if synced:
                print(f"[leadtime] stats_sync: updated manual leads for {synced} followers")
        except Exception as ex:
            print("[leadtime] stats_sync failed:", ex)
        return int(added or 0)
    except Exception:
        return 0


def ingest_status() -> dict:
    st = _read_state()
    cache_ev = _read_json(EVENTS_CACHE_PATH)
    rows = cache_ev.get("rows") or []
    total = len(rows)
    base_rows = sum(1 for e in rows if (e.get("sku") is None or int(e.get("sku") or 0) == 0))
    sku_rows = max(0, total - base_rows)

    last_run_at = str(st.get("last_run_at") or "")
    events_saved_at = _events_saved_at() or ""

    def _to_ts(s: str) -> float:
        d = _parse_iso_dt(s)
        return d.timestamp() if d else 0.0

    last_activity_iso = last_run_at
    if _to_ts(events_saved_at) > _to_ts(last_run_at):
        last_activity_iso = events_saved_at

    st_cache = _states_load()
    tracked = 0
    completed = 0
    for _, rec in st_cache.items():
        states = rec.get("states") or {}
        has_a = START_STATE in states
        has_c = _has_end_state(states)
        if has_a:
            tracked += 1
        if has_a and has_c:
            completed += 1

    return {
        "last_run_at": last_activity_iso,
        "last_added": int(st.get("last_added") or 0),
        "last_pages": int(st.get("last_pages") or 0),
        "total_cached": total,
        "base_rows": base_rows,
        "sku_rows": sku_rows,
        "tracked": tracked,
        "completed": completed,
        "in_progress": max(0, tracked - completed),
        "state_path": LEAD_INGEST_STATE_PATH,
        "events_path": EVENTS_CACHE_PATH,
        "states_path": STATES_CACHE_PATH,
    }


__all__ = [
    "get_stat_period",
    "save_stat_period",
    "get_lead_allocation_flag",
    "set_lead_allocation_flag",
    "_is_fresh",
    "_materialize_events",
    "_only_completed_with_duration",
    "_aggregate_stats",
    "_ensure_recent_events",
    "STATS_CACHE_PATH",
    "EVENTS_CACHE_PATH",
    "STATES_CACHE_PATH",
    "LEAD_STAT_DAYS_DEFAULT",
    "LEAD_STAT_TTL_HOURS",
    "LEAD_DISABLE_INGEST_ON_READ",
    "get_lead_stats_summary",
    "get_lead_stats_by_warehouse",
    "get_lead_stats_by_cluster",
    "get_lead_stats_by_sku",
    "get_lead_stats_sku_for_warehouse",
    "get_lead_stats_sku_for_cluster",
    "get_stats_avg_by_warehouse",
    "apply_stats_to_leads_for_followers",
    "update_leadtime_events",
    "ingest_tick",
    "ingest_status",
    "invalidate_stats_cache",
    "rebuild_events_from_states",
    "get_current_watch_sku",
]
